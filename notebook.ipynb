{"cells":[{"source":"# Build a Retrieval Augmented Generation (RAG) App","metadata":{},"cell_type":"markdown","id":"abd666df-97b0-4f0e-b110-142ce50ef61e"},{"source":"![RAG Indexing](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)","metadata":{},"cell_type":"markdown","id":"2196e32b-e99a-4a79-bebb-5db6899ee915"},{"source":"# Installation \nThis tutorial requires the `langchain` dependency.","metadata":{},"cell_type":"markdown","id":"cbd5f8da-9cda-4cef-b5cf-7fe1d06afa7e"},{"source":"%%capture\n%pip install --quiet --upgrade langchain langchain-community langchain-chroma","metadata":{"executionCancelledAt":null,"executionTime":9026,"lastExecutedAt":1732131498843,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"%%capture\n%pip install --quiet --upgrade langchain langchain-community langchain-chroma","outputsMetadata":{"0":{"height":479,"type":"stream"}}},"cell_type":"code","id":"548c3b6e-ac5f-4f48-824c-783aaeb47e49","outputs":[],"execution_count":8},{"source":"%%capture\n%pip install -qU langchain-openai","metadata":{"executionCancelledAt":null,"executionTime":3620,"lastExecutedAt":1732131502464,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"%%capture\n%pip install -qU langchain-openai","outputsMetadata":{"0":{"height":332,"type":"stream"}}},"cell_type":"code","id":"2bff181b-8ebb-415b-ab22-8141b9d0a580","outputs":[],"execution_count":9},{"source":"# Data Preprocessing Workflow","metadata":{},"cell_type":"markdown","id":"f1068e4f-4bd6-48a3-9775-bfcdc2fc9f11"},{"source":"transformer = {\n    \"transformer_id\":1,\n    \"Customer\": \"Honda Automobile (Thailand) Co., Ltd.\",\n    \"Location\": \"Ayutthaya Sub.1\",\n    \"Technical Data\": {\n        \"Manufacture\": \"DAIHEN\",\n        \"Type\": \"PP0318B02\",\n        \"Rated voltage\": \"115\",\n        \"Rated frequency\": \"50\",\n        \"Rated power\": \"20000/25000\"\n    },\n    \"On-Load Tap Changer\": {\n        \"Manufacture\": \"MR\",\n        \"Type\": \"Vlll 200D-123/76-1019 3W\",\n        \"Rated voltage\": \"400/230\",\n        \"Rated current\": \"161\"\n    }\n}","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1732131504545,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"transformer = {\n    \"transformer_id\":1,\n    \"Customer\": \"Honda Automobile (Thailand) Co., Ltd.\",\n    \"Location\": \"Ayutthaya Sub.1\",\n    \"Technical Data\": {\n        \"Manufacture\": \"DAIHEN\",\n        \"Type\": \"PP0318B02\",\n        \"Rated voltage\": \"115\",\n        \"Rated frequency\": \"50\",\n        \"Rated power\": \"20000/25000\"\n    },\n    \"On-Load Tap Changer\": {\n        \"Manufacture\": \"MR\",\n        \"Type\": \"Vlll 200D-123/76-1019 3W\",\n        \"Rated voltage\": \"400/230\",\n        \"Rated current\": \"161\"\n    }\n}"},"cell_type":"code","id":"1a45a36a-1535-41ce-b3f4-4efb53543dc0","outputs":[],"execution_count":10},{"source":"import pandas as pd\nfrom sqlalchemy import create_engine\n\n# Path to your Excel file\nfile_path = 'data/data_poc.xlsx'\n\n# SQLite database setup\nengine = create_engine('sqlite:///transformer_data.db')\n\n# Load the Excel file\nxls = pd.ExcelFile(file_path)\n\n# Process each sheet and save to SQLite\nfor sheet_name in xls.sheet_names:\n    # Load each sheet into a DataFrame\n    df = pd.read_excel(file_path, sheet_name=sheet_name)\n    \n    # Add transformer_id column\n    df['transformer_id'] = 1\n\n    # Clean the DataFrame (remove completely empty rows/columns)\n    df = df.dropna(how='all', axis=0)  # Remove empty rows\n    df = df.dropna(how='all', axis=1)  # Remove empty columns\n\n    # Save the DataFrame to SQLite\n    table_name = sheet_name.replace(\" \", \"_\").lower()  # Normalize table name\n    df.to_sql(table_name, engine, index=False, if_exists='replace')\n\nprint(\"Data from all sheets has been successfully saved to the SQLite database.\")\n","metadata":{"executionCancelledAt":null,"executionTime":8288,"lastExecutedAt":1732131515054,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom sqlalchemy import create_engine\n\n# Path to your Excel file\nfile_path = 'data/data_poc.xlsx'\n\n# SQLite database setup\nengine = create_engine('sqlite:///transformer_data.db')\n\n# Load the Excel file\nxls = pd.ExcelFile(file_path)\n\n# Process each sheet and save to SQLite\nfor sheet_name in xls.sheet_names:\n    # Load each sheet into a DataFrame\n    df = pd.read_excel(file_path, sheet_name=sheet_name)\n    \n    # Add transformer_id column\n    df['transformer_id'] = 1\n\n    # Clean the DataFrame (remove completely empty rows/columns)\n    df = df.dropna(how='all', axis=0)  # Remove empty rows\n    df = df.dropna(how='all', axis=1)  # Remove empty columns\n\n    # Save the DataFrame to SQLite\n    table_name = sheet_name.replace(\" \", \"_\").lower()  # Normalize table name\n    df.to_sql(table_name, engine, index=False, if_exists='replace')\n\nprint(\"Data from all sheets has been successfully saved to the SQLite database.\")\n","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"bfaa565f-966b-41d9-b8cb-e569febc62aa","outputs":[{"output_type":"stream","name":"stdout","text":"Data from all sheets has been successfully saved to the SQLite database.\n"}],"execution_count":11},{"source":"import sqlite3\n\n# Path to your SQLite database\ndb_path = 'transformer_data.db'\n\n# Connect to the database\nconn = sqlite3.connect(db_path)\n\n# Query to list all tables\nquery = \"SELECT name FROM sqlite_master WHERE type='table';\"\ncursor = conn.cursor()\ncursor.execute(query)\n\n# Fetch all table names\ntables = cursor.fetchall()\n\n# Print table names\nprint(\"Tables in the database:\")\nfor table in tables:\n    print(table[0])\n\n# Close the connection\nconn.close()\n","metadata":{"executionCancelledAt":null,"executionTime":79,"lastExecutedAt":1732131518481,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import sqlite3\n\n# Path to your SQLite database\ndb_path = 'transformer_data.db'\n\n# Connect to the database\nconn = sqlite3.connect(db_path)\n\n# Query to list all tables\nquery = \"SELECT name FROM sqlite_master WHERE type='table';\"\ncursor = conn.cursor()\ncursor.execute(query)\n\n# Fetch all table names\ntables = cursor.fetchall()\n\n# Print table names\nprint(\"Tables in the database:\")\nfor table in tables:\n    print(table[0])\n\n# Close the connection\nconn.close()\n","outputsMetadata":{"0":{"height":290,"type":"stream"}}},"cell_type":"code","id":"87487665-265a-495b-b26b-5cd0c4c6e059","outputs":[{"output_type":"stream","name":"stdout","text":"Tables in the database:\nt1\nt2\nt3\nt4\nt5\nt6\nt7\nt8\nt9\nt10\nt11\nt12\n"}],"execution_count":12},{"source":"import os\nimport sqlite3\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\n# Set up OpenAI API key\n# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"  # Replace with your OpenAI API Key\n\n# Connect to the SQLite database\ndb_path = \"transformer_data.db\"  # Path to your SQLite database\nconn = sqlite3.connect(db_path)\n\n# Function to load data from SQLite and create documents\ndef load_transformer_data(transformer_id):\n    query = f\"SELECT * FROM t1 WHERE transformer_id = {transformer_id}\"  # Adjust table name as needed\n    df = pd.read_sql_query(query, conn)\n\n    # Combine rows into a single document with context\n    transformer_docs = []\n    for _, row in df.iterrows():\n        doc_content = f\"\"\"\n        Transformer ID: {transformer_id}\n        Year: {row['year']}\n        HV to GND: {row.get('HV to GND', 'N/A')}\n        LV to GND: {row.get('LV to GND', 'N/A')}\n        HV to LV: {row.get('HV to LV', 'N/A')}\n        Measuring Type: {row.get('measuring_type', 'N/A')}\n        \"\"\"\n        transformer_docs.append(doc_content)\n    return transformer_docs\n\n# Load data for transformer_id = 1\ntransformer_id = 1\ndocs = load_transformer_data(transformer_id)\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_text(\"\\n\".join(docs))\n\n# Generate embeddings and store in Chroma\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_texts(splits, embeddings)\n\n# Set up the retriever\nretriever = vectorstore.as_retriever()\n\n# Define the RAG pipeline\nllm = OpenAI(temperature=0, model=\"gpt-4o\")\nprompt_template = \"\"\"\nYou are a transformer maintenance assistant. Based on the following transformer data:\n\n{context}\n\nAnswer the following question:\n{question}\n\"\"\"\nprompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n\nqa_chain = RetrievalQA(llm=llm, retriever=retriever, return_source_documents=True, prompt=prompt)\n\n# Query the RAG system\nquestion = \"What was the HV to GND value for 2561?\"\nresponse = qa_chain.run(question)\n\n# Display the response\nprint(\"Response:\")\nprint(response)\n","metadata":{"executionCancelledAt":null,"executionTime":972,"lastExecutedAt":1732131523109,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nimport sqlite3\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\n\n# Set up OpenAI API key\n# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"  # Replace with your OpenAI API Key\n\n# Connect to the SQLite database\ndb_path = \"transformer_data.db\"  # Path to your SQLite database\nconn = sqlite3.connect(db_path)\n\n# Function to load data from SQLite and create documents\ndef load_transformer_data(transformer_id):\n    query = f\"SELECT * FROM t1 WHERE transformer_id = {transformer_id}\"  # Adjust table name as needed\n    df = pd.read_sql_query(query, conn)\n\n    # Combine rows into a single document with context\n    transformer_docs = []\n    for _, row in df.iterrows():\n        doc_content = f\"\"\"\n        Transformer ID: {transformer_id}\n        Year: {row['year']}\n        HV to GND: {row.get('HV to GND', 'N/A')}\n        LV to GND: {row.get('LV to GND', 'N/A')}\n        HV to LV: {row.get('HV to LV', 'N/A')}\n        Measuring Type: {row.get('measuring_type', 'N/A')}\n        \"\"\"\n        transformer_docs.append(doc_content)\n    return transformer_docs\n\n# Load data for transformer_id = 1\ntransformer_id = 1\ndocs = load_transformer_data(transformer_id)\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_text(\"\\n\".join(docs))\n\n# Generate embeddings and store in Chroma\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_texts(splits, embeddings)\n\n# Set up the retriever\nretriever = vectorstore.as_retriever()\n\n# Define the RAG pipeline\nllm = OpenAI(temperature=0, model=\"gpt-4o\")\nprompt_template = \"\"\"\nYou are a transformer maintenance assistant. Based on the following transformer data:\n\n{context}\n\nAnswer the following question:\n{question}\n\"\"\"\nprompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n\nqa_chain = RetrievalQA(llm=llm, retriever=retriever, return_source_documents=True, prompt=prompt)\n\n# Query the RAG system\nquestion = \"What was the HV to GND value for 2561?\"\nresponse = qa_chain.run(question)\n\n# Display the response\nprint(\"Response:\")\nprint(response)\n","outputsMetadata":{"0":{"height":437,"type":"stream"}}},"cell_type":"code","id":"f32c37f6-b169-4391-9d71-c21340e8b2f8","outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 63\u001b[0m\n\u001b[1;32m     53\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124mYou are a transformer maintenance assistant. Based on the following transformer data:\u001b[39m\n\u001b[1;32m     55\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;132;01m{question}\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     61\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(template\u001b[38;5;241m=\u001b[39mprompt_template, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 63\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_source_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Query the RAG system\u001b[39;00m\n\u001b[1;32m     66\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat was the HV to GND value for 2561?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:183\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     emit_warning()\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n","\u001b[0;31mValidationError\u001b[0m: 3 validation errors for RetrievalQA\ncombine_documents_chain\n  field required (type=value_error.missing)\nllm\n  extra fields not permitted (type=value_error.extra)\nprompt\n  extra fields not permitted (type=value_error.extra)"],"ename":"ValidationError","evalue":"3 validation errors for RetrievalQA\ncombine_documents_chain\n  field required (type=value_error.missing)\nllm\n  extra fields not permitted (type=value_error.extra)\nprompt\n  extra fields not permitted (type=value_error.extra)"}],"execution_count":4},{"source":"import getpass\nimport os\nimport bs4\nos.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\nfrom langchain import hub\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n# Load, chunk and index the contents of the blog.\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# Retrieve and generate using the relevant snippets of the blog.\nretriever = vectorstore.as_retriever()\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nrag_chain.invoke(\"What is Task Decomposition?\")","metadata":{"executionCancelledAt":null,"executionTime":367,"lastExecutedAt":1732131530767,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import getpass\nimport os\nimport bs4\nos.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\")\nfrom langchain import hub\nfrom langchain_chroma import Chroma\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\n# Load, chunk and index the contents of the blog.\nloader = WebBaseLoader(\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n    bs_kwargs=dict(\n        parse_only=bs4.SoupStrainer(\n            class_=(\"post-content\", \"post-title\", \"post-header\")\n        )\n    ),\n)\ndocs = loader.load()\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\nvectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n\n# Retrieve and generate using the relevant snippets of the blog.\nretriever = vectorstore.as_retriever()\nprompt = hub.pull(\"rlm/rag-prompt\")\n\n\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nrag_chain.invoke(\"What is Task Decomposition?\")"},"cell_type":"code","id":"2d86d91d-0753-4417-a42a-35b36c5ce2eb","outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_parsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnablePassthrough\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI, ChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAIEmbeddings, OpenAIEmbeddings\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI, OpenAI\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/chat_models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureChatOpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureChatOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_openai/chat_models/azure.py:21\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     Any,\n\u001b[1;32m      9\u001b[0m     Awaitable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     Union,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LangSmithParams\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseMessage\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatResult\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py)"],"ename":"ImportError","evalue":"cannot import name 'LangSmithParams' from 'langchain_core.language_models.chat_models' (/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py)"}],"execution_count":null},{"source":"# cleanup\nvectorstore.delete_collection()","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1732131556609,"lastExecutedByKernel":"878ed515-3c52-4624-9b2e-6b0611ba457a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# cleanup\nvectorstore.delete_collection()"},"cell_type":"code","id":"4d26ece6-07da-4703-88be-a879648d05e0","outputs":[],"execution_count":15}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}